version: 1.3.1
cache: true

interface:
  customWelcome: '$${LIBRECHAT_CUSTOM_WELCOME}'
  privacyPolicy:
    externalUrl: '$${LIBRECHAT_PRIVACY_POLICY_URL}'
    openNewTab: true
  termsOfService:
    externalUrl: '$${LIBRECHAT_TERMS_OF_SERVICE_URL}'
    openNewTab: true
    modalAcceptance: true
    modalTitle: 'Kleingedrucktes'
    modalContent: |
      # Nutzungsbedingungen

      **Wichtiger Hinweis zu KI-generierten Antworten**
      Alle Antworten dieses KI-Systems sollten stets von Menschen überprüft werden. KI-generierte Inhalte können Fehler enthalten und sollten nicht als alleinige Quelle für wichtige Entscheidungen verwendet werden.

      **Datenverarbeitung und Hosting**
      Wir bemühen uns, so viele Komponenten wie möglich selbst zu hosten. Aktuell nutzen wir noch externe Dienste wie OpenRouter, wodurch Daten derzeit an Server in den USA übertragen werden. Unser Ziel ist es, vollständig auf selbst gehostete oder europäische Dienste umzustellen.
  endpointsMenu: true
  modelSelect: true
  parameters: true
  sidePanel: true
  presets: true
  prompts: true
  bookmarks: true
  multiConvo: true
  agents: true
  peoplePicker:
    users: true
    groups: true
    roles: true
  marketplace:
    use: true
  fileCitations: true
  fileSearch: true
  search: true

memory:
  disabled: false
  personalize: true
  tokenLimit: 2000
  messageWindowSize: 5
  # Memory categories for journalism and fact-checking workflow
  # Restricts memory storage to these structured categories for better consistency
  validKeys:
    # User preferences and communication style
    - "preferences"
    # Work-related: role, organization, department, responsibilities
    - "work_info"
    # Personal information: name, location, contact preferences
    - "personal_info"
    # Current projects, research topics, ongoing investigations
    - "projects"
    # Fact-checking specific: methods, verification approaches, source preferences
    - "factcheck_info"
    # Research context: background info, historical context, related investigations
    - "research_context"
    # Preferred sources, verification methods, trusted outlets
    - "sources"
    # Output format preferences: article style, citation format, structure
    - "output_format"
    # Important deadlines, publication dates, milestones
    - "deadlines"
    # Expertise areas, specializations, domain knowledge
    - "expertise"
    # Final results, conclusions, verified facts from completed fact-checks
    - "results"
  agent:
    provider: "Scaleway"
    # European Open Source model optimized for Memory tasks with cost-effective function calling
    # mistral-small-3.2-24b-instruct-2506: Cost-effective (3-6x cheaper than 70B models), good function calling
    # Supports 128K context window, improved function calling in v3.2, European-hosted
    model: "mistral-small-3.2-24b-instruct-2506"
    instructions: |
      Store reusable info via validKeys: preferences, work_info, personal_info, projects, factcheck_info, research_context, sources, output_format, deadlines, expertise, results. NEVER: greetings, casual chat, trivial/obvious statements. PRIORITY: Explicit user requests ("Merke dir", "Remember", "Speichere") → always store, even if minor. If only greetings/casual/no info AND no explicit request → END without tools. UPDATE: Check existing memories first. If match exists → UPDATE via `set_memory` (merge old+new). Only create new if no match. NEVER delete+recreate. Delete outdated/corrected promptly. Max 2000 tokens/value.
    model_parameters:
      # Low temperature (0.15) for precise, deterministic function calling decisions
      # Mistral Small 3.2 performs best with temp 0.15 for reliable structured output
      temperature: 0.15
      # top_p: 0.95 provides controlled sampling while maintaining flexibility
      # Optimal for Mistral models when paired with low temperature
      top_p: 0.95

endpoints:
  # Agents configuration
  agents:
    disableBuilder: false
    # Vision: WIP/experimental. Requires feat/vision in dev/librechat + dev/agents. Draft PRs: LibreChat #11501 (https://github.com/danny-avila/LibreChat/pull/11501), agents #48 (https://github.com/danny-avila/agents/pull/48). See docs/wip/README.md
    capabilities:
      - "execute_code"
      - "file_search"
      - "actions"
      - "artifacts"
      - "chain"
      - "context"
      - "ocr"
      - "tools"
      - "web_search"
      - "vision"   # WIP/experimental

  custom:
    - name: OpenRouter
      apiKey: "$${OPENROUTER_KEY}"
      baseURL: "$${OPENROUTER_BASE_URL}"
      models:
        # Premium models only - Fallback list if fetch fails or fetch is disabled
        # Focus on top-performance proprietary models for special use cases
        # Note: Open Source models should use Scaleway (Europa) for better privacy
        default: [
          "anthropic/claude-sonnet-4.5",      # Top proprietary model (Vision, 200K Context)
          "openai/gpt-5.2",                   # Latest GPT-5 series (Vision, 400K Context)
          "google/gemini-2.5-flash-lite",    # Google's lightweight reasoning model (Vision)
          "mistralai/mistral-large-2411",     # European premium option (French)
          "glm/glm-4.7"                       # Advanced reasoning model
        ]
        # Default: fetch all models from API. Production override (librechat.prod.yaml) sets fetch: false.
        fetch: true
      titleConvo: true
      summarize: true
      headers:
        HTTP-Referer: "$${OPENROUTER_SITE_URL}"
        X-Title: "$${OPENROUTER_APP_NAME}"

    - name: Scaleway
      apiKey: "$${SCALEWAY_API_KEY}"
      baseURL: "$${SCALEWAY_BASE_URL}"
      iconURL: "https://www.scaleway.com/favicon/website/favicon.svg"
      # Scaleway docs: "Meta models do not support parallel tool calls."
      # Affected: Llama 3.1 (8b, 70b), Llama 3.3 (70b). Without this → 400 single tool-calls error.
      # See docs/LIBRECHAT_FEATURES.md#scaleway-parallel-tool-calls.
      addParams:
        parallel_tool_calls: false
      # Scaleway Chat Completions API does not support these params; strip to avoid errors.
      # See docs/LIBRECHAT_FEATURES.md#scaleway-unsupported-parameters.
      dropParams:
        - frequencyPenalty   # frequency_penalty not supported
        - n                  # multiple completions not supported
        - topLogprobs        # top_logprobs not supported
        - logitBias          # logit_bias not supported
        - user               # user identifier not supported
      # BASE_URL construction: If SCALEWAY_PROJECT_ID is set, the initialization script
      # automatically constructs the BASE_URL as https://api.scaleway.ai/{project_id}/v1.
      # Otherwise, the default SCALEWAY_BASE_URL value is used.
      models:
        default: [
          "llama-3.3-70b-instruct",           # Chat - Generally Available
          "qwen3-235b-a22b-instruct-2507",    # Chat - Generally Available
          "mistral-small-3.2-24b-instruct-2506", # Chat, Vision - Generally Available
          "gpt-oss-120b",                      # Chat - Generally Available
          "qwen3-coder-30b-a3b-instruct",     # Chat (Code) - Generally Available
          "pixtral-12b-2409",                 # Chat, Vision - Generally Available
          "qwen3-embedding-8b",               # Embeddings - Generally Available
          "bge-multilingual-gemma2"           # Embeddings - Generally Available
        ]
        # Default: fetch all models from API. Production override (librechat.prod.yaml) sets fetch: false.
        fetch: true
      titleConvo: true
      summarize: true
      headers:
        HTTP-Referer: "$${SCALEWAY_SITE_URL}"
        X-Title: "$${SCALEWAY_APP_NAME}"

# Model Specs: Define default model for new users
# This prevents "My Agents" from being selected when users have no agents
# Note: Memory function uses a separate model (configured in memory.agent section)
# Chat models don't need Function Calling for Memory - Memory works independently
#
# ==========================================
# MODELL-ANALYSE UND GRUPPIERUNG
# ==========================================
#
# GRUPPE 1: "Empfohlen: Europa & Open Source" (Scaleway)
# ------------------------------------------
# Alle Modelle in dieser Gruppe sind:
# - Open Source (Apache-2.0, Llama Community, MIT, etc.)
# - In Europa gehostet (Scaleway Data Centers)
# - DSGVO-konform
# - Optimal für datenschutzkritische Anwendungen
#
# DETAILLIERTE MODELL-ANALYSE (Scaleway):
#
# CHAT MODELLE (Text-only):
# -------------------------
# 1. llama-3.3-70b-instruct (Meta)
#    - Context: 100K, Output: 4K
#    - License: Llama 3.3 Community
#    - Use Case: Allgemeine Gespräche, gute Balance
#    - Status: Generally Available, empfohlen von Scaleway
#
# 2. qwen3-235b-a22b-instruct-2507 (Qwen)
#    - Context: 250K (größter verfügbarer Context!), Output: 8K
#    - License: Apache-2.0
#    - Use Case: Sehr komplexe Aufgaben, extrem lange Kontexte
#    - Status: Generally Available
#
# 3. qwen3-coder-30b-a3b-instruct (Qwen)
#    - Context: 128K, Output: 8K
#    - License: Apache-2.0
#    - Use Case: Programmierung, Code-Generierung, Code-Analyse
#    - Status: Generally Available
#
# 4. gpt-oss-120b (OpenAI Open Source)
#    - Context: 128K, Output: 8K
#    - License: Apache-2.0
#    - Use Case: Komplexe Aufgaben, GPT-kompatible API
#    - Status: Generally Available
#
# CHAT & VISION MODELLE:
# ----------------------
# 5. mistral-small-3.2-24b-instruct-2506 (Mistral)
#    - Context: 128K, Output: 8K
#    - License: Apache-2.0
#    - Use Case: Kosteneffektiv (3-6x günstiger als 70B), Vision, allgemeine Aufgaben
#    - Status: Generally Available, EMPFOHLEN von Scaleway als Einstiegsmodell
#    - Besonderheit: Beste Kosten-Nutzen-Balance
#
# 6. pixtral-12b-2409 (Mistral)
#    - Context: 128K, Output: 4K
#    - License: Apache-2.0
#    - Use Case: Spezialisiert auf Bildverständnis, Bilder bis 32M Pixel
#    - Status: Generally Available
#
# DEPRECATED MODELLE (nicht mehr verwenden):
# - deepseek-r1-distill-llama-70b (EOL: 16. Apr 2026)
# - mistral-nemo-instruct-2407 (EOL: 16. Apr 2026)
# - llama-3.1-8b-instruct (EOL: 16. Apr 2026)
#
# GRUPPE 2: "Premium-Modelle" (OpenRouter)
# ------------------------------------------
# Diese Modelle sind:
# - Top-Performance für spezielle Anwendungsfälle
# - Closed Source (Anthropic, OpenAI, Google) oder spezielle Zugänge erforderlich
# - Über OpenRouter gehostet (USA)
# - Nur für nicht-datenschutzkritische Aufgaben empfohlen
#
# DETAILLIERTE MODELL-ANALYSE (OpenRouter Premium):
#
# 1. claude-sonnet-4.5 (Anthropic)
#    - Context: 200K, Output: 64K
#    - License: Closed Source (Proprietary)
#    - Use Case: Top-Performance für komplexe Aufgaben, Programmierung, Reasoning
#    - Vision: Ja
#    - Pricing: $3/M input, $15/M output tokens
#    - Besonderheit: Exzellentes Reasoning, sehr gute Code-Generierung
#
# 2. openai/gpt-5.2 (OpenAI)
#    - Context: 400K (größter Context!), Output: 128K
#    - License: Closed Source (Proprietary)
#    - Use Case: Top-Performance, Dynamic Reasoning, komplexe Problemlösung
#    - Vision: Ja
#    - Pricing: $1.25/M input, $10/M output tokens
#    - Besonderheit: ~80% weniger Halluzinationen als o3, 74.9% SWE-Bench accuracy
#    - Hinweis: Erfordert eigenen OpenAI API Key
#
# 3. google/gemini-2.5-flash-lite (Google)
#    - Context: Variabel, Output: Variabel
#    - License: Closed Source (Proprietary)
#    - Use Case: Schnell und kostengünstig, gute Qualität
#    - Vision: Ja
#    - Besonderheit: Optimiert für Geschwindigkeit bei guter Qualität
#
# 4. mistralai/mistral-large-2411 (Mistral)
#    - Context: Variabel, Output: Variabel
#    - License: Closed Source (Proprietary)
#    - Use Case: Premium-Modell, europäisches Unternehmen
#    - Besonderheit: Französisches Unternehmen, beliebt bei europäischen Nutzern
#
# 5. glm/glm-4.7 (Z.AI)
#    - Context: Variabel, Output: Variabel
#    - License: Closed Source (Proprietary)
#    - Use Case: Exzellentes logisches Denken, komplexe Reasoning-Aufgaben
#    - Besonderheit: Fortgeschrittenes Sprachverständnis
#
# EMPFEHLUNGEN NACH ANWENDUNGSFALL:
# ----------------------------------
# Datenschutzkritisch → Scaleway Modelle (Europa, Open Source)
#   - Allgemein: mistral-small-3.2-24b (kosteneffektiv, Vision)
#   - Komplex: llama-3.3-70b oder qwen3-235b
#   - Code: qwen3-coder-30b
#   - Vision: pixtral-12b oder mistral-small-3.2-24b
#
# Top Performance benötigt → Premium-Modelle (OpenRouter)
#   - Best Overall: claude-sonnet-4.5
#   - Largest Context: openai/gpt-5.2 (400K)
#   - Fast & Cheap: google/gemini-2.5-flash-lite
#   - European Premium: mistral-large-2411
#   - Reasoning: glm/glm-4.7
#
# ==========================================
modelSpecs:
  list:
    # ==========================================
    # Empfohlen: Europa & Open Source (Scaleway) – alphabetisch nach Label
    # ==========================================
    # Alle Modelle: Open Source, Europa-gehostet, DSGVO-konform
    # Token limits: Scaleway Generative APIs + Managed Inference model catalog (Oct 2025)
    - name: "scaleway-devstral-2-123b"
      label: "Devstral 2 123B"
      description: "Coding- und Agent-Modell (Mistral). 200K Context, 8K Output. Ideal für Code, Tools und Software-Agents. Modified MIT."
      group: "Empfohlen: Europa & Open Source"
      groupIcon: "data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9ImN1cnJlbnRDb2xvciIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiIGNsYXNzPSJsdWNpZGUgbHVjaWRlLWxvY2staWNvbiBsdWNpZGUtbG9jayI+PHJlY3Qgd2lkdGg9IjE4IiBoZWlnaHQ9IjExIiB4PSIzIiB5PSIxMSIgcng9IjIiIHJ5PSIyIi8+PHBhdGggZD0iTTcgMTFWN2E1IDUgMCAwIDEgMTAgMHY0Ii8+PC9zdmc+"
      order: 1
      preset:
        endpoint: "Scaleway"
        endpointType: "custom"
        model: "devstral-2-123b-instruct-2512"
        maxContextTokens: 200000
        maxOutputTokens: 8192

    - name: "scaleway-gemma-3-27b"
      label: "Gemma 3 27B"
      description: "Kompaktes Chat- und Vision-Modell (Google). 40K Context, 8K Output. Gut für Bildanalyse und allgemeine Aufgaben. Gemma License, derzeit Preview bei Scaleway."
      group: "Empfohlen: Europa & Open Source"
      order: 2
      vision: true
      preset:
        endpoint: "Scaleway"
        endpointType: "custom"
        model: "gemma-3-27b-it"
        maxContextTokens: 40000
        maxOutputTokens: 8192

    - name: "scaleway-gpt-oss-120b"
      label: "GPT-OSS 120B"
      description: "Großes Modell für komplexe Aufgaben. 128K Context, 8K Output. Apache-2.0 License."
      group: "Empfohlen: Europa & Open Source"
      order: 3
      preset:
        endpoint: "Scaleway"
        endpointType: "custom"
        model: "gpt-oss-120b"
        maxContextTokens: 128000
        maxOutputTokens: 8192

    - name: "scaleway-holo2-30b"
      label: "Holo2 30B"
      description: "Chat- und Vision-Modell mit Fokus auf GUI-Analyse (Browser, Software-Oberflächen). 22K Context, 8K Output. CC-BY-NC-4.0; Nutzung über Scaleway erlaubt. Zum Testen von UI/UX-Verständnis."
      group: "Empfohlen: Europa & Open Source"
      order: 4
      vision: true
      preset:
        endpoint: "Scaleway"
        endpointType: "custom"
        model: "holo2-30b-a3b"
        maxContextTokens: 22000
        maxOutputTokens: 8192

    - name: "scaleway-llama-3.3-70b"
      label: "Llama 3.3 70B"
      description: "Meta's neuestes Modell. 100K Context, 4K Output. Sehr gut für allgemeine Gespräche und Aufgaben. Llama 3.3 Community License. Empfohlen von Scaleway."
      group: "Empfohlen: Europa & Open Source"
      order: 5
      preset:
        endpoint: "Scaleway"
        endpointType: "custom"
        model: "llama-3.3-70b-instruct"
        maxContextTokens: 100000   # Generative APIs; Managed Inference up to 128k (instance-dependent)
        maxOutputTokens: 4096

    - name: "scaleway-mistral-small-3.2"
      label: "Mistral Small 3.2 24B"
      description: "Kosteneffektiv (3-6x günstiger als 70B Modelle). 128K Context, 8K Output. Unterstützt Chat und Vision. Apache-2.0 License. Empfohlen von Scaleway als Einstiegsmodell."
      group: "Empfohlen: Europa & Open Source"
      order: 6
      vision: true
      default: true
      preset:
        endpoint: "Scaleway"
        endpointType: "custom"
        model: "mistral-small-3.2-24b-instruct-2506"
        maxContextTokens: 128000
        maxOutputTokens: 8192

    - name: "scaleway-pixtral-12b"
      label: "Pixtral 12B"
      description: "Spezialisiert auf Bildverständnis und Chat. 128K Context, 4K Output. Unterstützt Bilder bis 32M Pixel. Apache-2.0 License."
      group: "Empfohlen: Europa & Open Source"
      order: 7
      vision: true
      preset:
        endpoint: "Scaleway"
        endpointType: "custom"
        model: "pixtral-12b-2409"
        maxContextTokens: 128000
        maxOutputTokens: 4096

    - name: "scaleway-qwen3-235b"
      label: "Qwen3 235B"
      description: "Sehr großes Modell (235B Parameter). 250K Context, 8K Output. Optimal für komplexe Aufgaben und lange Kontexte. Apache-2.0 License."
      group: "Empfohlen: Europa & Open Source"
      order: 8
      preset:
        endpoint: "Scaleway"
        endpointType: "custom"
        model: "qwen3-235b-a22b-instruct-2507"
        maxContextTokens: 250000
        maxOutputTokens: 8192

    - name: "scaleway-qwen3-coder-30b"
      label: "Qwen3 Coder 30B"
      description: "Spezialisiert auf Programmierung und Code-Generierung. 128K Context, 8K Output. Apache-2.0 License."
      group: "Empfohlen: Europa & Open Source"
      order: 9
      preset:
        endpoint: "Scaleway"
        endpointType: "custom"
        model: "qwen3-coder-30b-a3b-instruct"
        maxContextTokens: 128000
        maxOutputTokens: 8192

    - name: "scaleway-voxtral-small-24b"
      label: "Voxtral Small 24B"
      description: "Chat- und Audio-Modell (Mistral). 32K Context, 8K Output. Transkription und Verstehen von Audio (WAV, MP3), mehrsprachig. Apache-2.0 License. Zum Testen der Audio-Fähigkeiten."
      group: "Empfohlen: Europa & Open Source"
      order: 10
      preset:
        endpoint: "Scaleway"
        endpointType: "custom"
        model: "voxtral-small-24b-2507"
        maxContextTokens: 32000
        maxOutputTokens: 8192

    # ==========================================
    # Premium-Modelle (OpenRouter) – alphabetisch nach Label
    # ==========================================
    # Premium-Modelle: Top-Performance Modelle für spezielle Anwendungsfälle
    # Diese Modelle sind Closed Source oder erfordern spezielle Zugänge
    # Hosted über OpenRouter (USA), daher nur für nicht-datenschutzkritische Aufgaben
    # Token limits: OpenRouter API / provider docs (context_length, max_completion_tokens)
    - name: "claude-opus-4.5"
      label: "Claude Opus 4.5"
      description: "Anthropics leistungsstärkstes Modell. Optimiert für komplexe Software-Engineering-, Agent- und Reasoning-Aufgaben. 200K Context, 64K Output. Vision, erweiterte Tool-Nutzung. Closed Source (Anthropic)."
      group: "Premium-Modelle"
      groupIcon: "data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIyNCIgaGVpZ2h0PSIyNCIgdmlld0JveD0iMCAwIDI0IDI0IiBmaWxsPSJub25lIiBzdHJva2U9ImN1cnJlbnRDb2xvciIgc3Ryb2tlLXdpZHRoPSIyIiBzdHJva2UtbGluZWNhcD0icm91bmQiIHN0cm9rZS1saW5lam9pbj0icm91bmQiIGNsYXNzPSJsdWNpZGUgbHVjaWRlLWJvdC1pY29uIGx1Y2lkZS1ib3QiPjxwYXRoIGQ9Ik0xMiA4VjRIOCIvPjxyZWN0IHdpZHRoPSIxNiIgaGVpZ2h0PSIxMiIgeD0iNCIgeT0iOCIgcng9IjIiLz48cGF0aCBkPSJNMiAxNGgyIi8+PHBhdGggZD0iTTIwIDE0aDIiLz48cGF0aCBkPSJNMTUgMTN2MiIvPjxwYXRoIGQ9Ik05IDEzdjIiLz48L3N2Zz4="
      order: 1
      vision: true
      preset:
        endpoint: "OpenRouter"
        endpointType: "custom"
        model: "anthropic/claude-opus-4.5"
        maxContextTokens: 200000
        maxOutputTokens: 64000

    - name: "claude-sonnet-4.5"
      label: "Claude Sonnet 4.5"
      description: "Top-Performance Modell für komplexe Aufgaben, Programmierung und Reasoning. 200K Context, 64K Output. Unterstützt Vision. Closed Source (Anthropic)."
      group: "Premium-Modelle"
      order: 2
      vision: true
      preset:
        endpoint: "OpenRouter"
        endpointType: "custom"
        model: "anthropic/claude-sonnet-4.5"
        maxContextTokens: 200000
        maxOutputTokens: 64000

    - name: "deepseek-v3.2"
      label: "DeepSeek V3.2"
      description: "Starkes Reasoning- und Tool-Use-Modell. 164K Context, 65K Output. Gutes Preis-Leistungs-Verhältnis für Coding und mehrstufige Aufgaben. Open Weights (DeepSeek)."
      group: "Premium-Modelle"
      order: 3
      preset:
        endpoint: "OpenRouter"
        endpointType: "custom"
        model: "deepseek/deepseek-v3.2"
        maxContextTokens: 163840
        maxOutputTokens: 65536

    - name: "google-gemini-2.5-flash-lite"
      label: "Gemini 2.5 Flash Lite"
      description: "Schnell und kostengünstig. Optimiert für schnelle Antworten bei guter Qualität. Unterstützt Vision. Closed Source (Google)."
      group: "Premium-Modelle"
      order: 4
      vision: true
      preset:
        endpoint: "OpenRouter"
        endpointType: "custom"
        model: "google/gemini-2.5-flash-lite"
        maxContextTokens: 1048576
        maxOutputTokens: 65536

    - name: "google-gemini-3-pro-preview"
      label: "Gemini 3 Pro Preview"
      description: "Googles aktuelles Spitzenmodell. 1M Context, Text/Image/Audio/Video. Starke Multimodal- und Reasoning-Fähigkeiten, Agent-Coding. Closed Source (Google)."
      group: "Premium-Modelle"
      order: 5
      vision: true
      preset:
        endpoint: "OpenRouter"
        endpointType: "custom"
        model: "google/gemini-3-pro-preview"
        maxContextTokens: 1048576
        maxOutputTokens: 65536

    - name: "z-ai-glm-4.7"
      label: "GLM 4.7"
      description: "Fortgeschrittenes Modell mit exzellentem logischen Denken. Sehr gut für komplexe Aufgaben und Reasoning. 203K Context, 65K Output. Closed Source (Z.AI)."
      group: "Premium-Modelle"
      order: 6
      preset:
        endpoint: "OpenRouter"
        endpointType: "custom"
        model: "glm/glm-4.7"
        maxContextTokens: 202752
        maxOutputTokens: 65535

    - name: "openai-gpt5-2"
      label: "GPT-5.2"
      description: "OpenAI's neuestes Top-Modell. 400K Context, 128K Output. Dynamic Reasoning, ~80% weniger Halluzinationen als o3. Unterstützt Vision. Closed Source (OpenAI)."
      group: "Premium-Modelle"
      order: 7
      vision: true
      preset:
        endpoint: "OpenRouter"
        endpointType: "custom"
        model: "openai/gpt-5.2"
        maxContextTokens: 400000
        maxOutputTokens: 128000

    - name: "openai-gpt5-2-pro"
      label: "GPT-5.2 Pro"
      description: "OpenAI's leistungsstärkstes Modell für anspruchsvolle Reasoning- und Agent-Aufgaben. 400K Context, 128K Output. Starke Verbesserungen bei Coding und Langkontext. Vision. Closed Source (OpenAI)."
      group: "Premium-Modelle"
      order: 8
      vision: true
      preset:
        endpoint: "OpenRouter"
        endpointType: "custom"
        model: "openai/gpt-5.2-pro"
        maxContextTokens: 400000
        maxOutputTokens: 128000

    - name: "mistral-large-2411"
      label: "Mistral Large 2411"
      description: "Premium-Modell von Mistral (französisches Unternehmen). Sehr beliebt bei europäischen Nutzern. Gute Balance aus Performance und Datenschutz. 256K Context, 8K Output."
      group: "Premium-Modelle"
      order: 9
      preset:
        endpoint: "OpenRouter"
        endpointType: "custom"
        model: "mistralai/mistral-large-2411"
        maxContextTokens: 256000
        maxOutputTokens: 8192

    - name: "mistral-large-2512"
      label: "Mistral Large 3 2512"
      description: "Mistrals aktuellstes Großmodell (675B MoE, 41B aktiv). 262K Context. Text + Vision. Apache 2.0. Gute Option für Europa-relevante Premium-Aufgaben."
      group: "Premium-Modelle"
      order: 10
      vision: true
      preset:
        endpoint: "OpenRouter"
        endpointType: "custom"
        model: "mistralai/mistral-large-2512"
        maxContextTokens: 262144
        maxOutputTokens: 8192


webSearch:
  enabled: true
  searchProvider: "searxng"
  searxngInstanceUrl: "$${LIBRECHAT_SEARXNG_URL}"
  searxngApiKey: "$${LIBRECHAT_SEARXNG_API_KEY}"
  scraperProvider: "firecrawl"
  firecrawlApiKey: "$${FIRECRAWL_API_KEY}"
  firecrawlApiUrl: "$${FIRECRAWL_API_URL}"
  firecrawlVersion: "$${FIRECRAWL_VERSION}"
  jinaApiKey: "$${LIBRECHAT_JINA_API_KEY}"
  jinaApiUrl: "$${LIBRECHAT_JINA_API_URL}"
  rerankerType: "jina"
  scraperTimeout: 7500
  safeSearch: 1

registration:
  allowedDomains:
    - correctiv.org
    - faktenforum.org
  # Optional: Social Login Providers (default: all disabled)
  # socialLogins: ['github', 'google', 'discord', 'openid', 'facebook', 'apple', 'saml']

# Balance System (disabled - for cost-based limits)
# balance:
#   enabled: false
#   startBalance: 20000        # Starting balance for new users
#   autoRefillEnabled: false   # Automatic refill
#   refillIntervalValue: 30    # Interval value
#   refillIntervalUnit: 'days' # Interval unit (days, weeks, months)
#   refillAmount: 10000        # Refill amount

# Transactions (disabled - for transaction logs)
# Default: true (enabled), automatically enabled when balance.enabled: true
# transactions:
#   enabled: false

# Speech: STT via Scaleway (requires SCALEWAY_PROJECT_ID + SCALEWAY_API_KEY; init injects URL)
speech:
  stt:
    openai:
      url: '$${SCALEWAY_STT_URL}'
      apiKey: '${SCALEWAY_API_KEY}'
      model: 'whisper-large-v3'
  # tts: browser or later OpenAI TTS_API_KEY

# Cloudflare Turnstile CAPTCHA (disabled)
# turnstile:
#   siteKey: "your-site-key-here"
#   options:
#     language: "auto"    # "auto" or ISO 639-1 code (e.g. "de")
#     size: "normal"      # "normal", "compact", "flexible", "invisible"

# Rate Limits (disabled - for upload/import limits)
# rateLimits:
#   fileUploads:
#     ipMax: 100                    # Max uploads per IP
#     ipWindowInMinutes: 60         # Time window for IP limit
#     userMax: 50                   # Max uploads per user
#     userWindowInMinutes: 60       # Time window for user limit
#   conversationsImport:
#     ipMax: 100                    # Max imports per IP
#     ipWindowInMinutes: 60         # Time window for IP limit
#     userMax: 50                   # Max imports per user
#     userWindowInMinutes: 60       # Time window for user limit

# File Config (disabled - granular upload configuration)
# fileConfig:
#   endpoints:
#     assistants:
#       fileLimit: 5                # Max files per request
#       fileSizeLimit: 10           # Max size per file (MB)
#       totalSizeLimit: 50          # Max total size (MB)
#       supportedMimeTypes:
#         - "image/.*"
#         - "application/pdf"
#     openAI:
#       disabled: true              # Disables uploads for OpenAI endpoint
#     default:
#       totalSizeLimit: 20          # Default total size (MB)
#   serverFileSizeLimit: 100        # Global server limit (MB)
#   avatarSizeLimit: 2              # Avatar size limit (MB)
#   imageGeneration:
#     percentage: 100               # Image size in percentage
#     px: 1024                      # Image size in pixels
#   clientImageResize:
#     enabled: false                # Client-side image compression
#     maxWidth: 1900                # Max width (px)
#     maxHeight: 1900               # Max height (px)
#     quality: 0.92                 # JPEG quality (0.0-1.0)

# MCP Servers Interface Config (default: use: true, create: true, share: false)
# interface:
#   mcpServers:
#     use: true       # Users can use MCP servers (default: true)
#     create: true    # Users can create MCP servers (default: true)
#     share: false    # Users can share MCP servers (default: false)

# Temporary Chat Retention (disabled - automatic deletion)
# interface:
#   temporaryChatRetention: 720  # Hours (min: 1, max: 8760, default: 720 = 30 days)

# Actions Domain Restrictions (disabled - SSRF protection for Agent Actions)
# SECURITY: If not configured, SSRF targets are blocked (localhost, private IPs, .internal/.local TLDs)
# To allow internal targets, they MUST be explicitly added to allowedDomains
# Supports wildcards: '*.example.com' and protocol/port restrictions: 'https://api.example.com:8443'
# actions:
#   allowedDomains:
#     - 'swapi.dev'
#     - 'librechat.ai'
#     - 'google.com'
#     # - 'http://10.225.26.25:7894'  # Internal IP with protocol/port (uncomment if needed)

# MCP Settings (SSRF protection for MCP Server Remote Transports)
# SECURITY: If not configured, SSRF targets are blocked (localhost, private IPs, .internal/.local TLDs)
# To allow internal targets like host.docker.internal, they MUST be explicitly added
# Supports wildcards: '*.example.com' matches 'api.example.com', 'staging.example.com', etc.
# Supports protocol/port restrictions: 'https://api.example.com:8443'
mcpSettings:
  allowedDomains:
    - 'host.docker.internal'    # Docker host access (required for Docker setups)
    - 'localhost'               # Local development
    - 'mcp-calculator'           # MCP Calculator service (Docker network)
    - 'mcp-image-gen'            # MCP Image Generation service (Docker network)
    - 'mcp-firecrawl'           # MCP Firecrawl service (Docker network)
    - 'mcp-openstreetmap'       # MCP OpenStreetMap service (Docker network)
    - 'mcp-weather'             # MCP Weather service (Docker network)
    - 'mcp-playwright'          # MCP Playwright service (Docker network)
    - 'mcp-db-timetable'        # MCP DB Timetable service (Docker network)
    - 'mcp-stackoverflow'       # MCP StackOverflow service (Docker network)
    - 'mcp-npm-search'           # MCP npm Search service (Docker network)
    - 'mcp-ytptube'             # MCP YTPTube (YTPTube + Scaleway STT)
    - 'mcp-youtube-transcript'  # MCP YouTube Transcript (youtube-transcript-api)
    - 'mcp-docs'                # MCP Docs (Grounded Docs) service (Docker network)
    - 'mcp-chefkoch'            # MCP Chefkoch service (Docker network)
    - 'api.githubcopilot.com'   # GitHub MCP Server (remote hosted)
    - 'mcp.mapbox.com'           # Mapbox MCP Server (remote hosted)
    # - '*.example.com'           # Wildcard subdomain
    # - 'https://secure.api.com'  # Protocol-restricted
    # - 'http://internal:8080'    # Protocol and port restricted
    # n8n Integration (uncomment when MCP bridge is ready)
    # - 'http://n8n:5678'        # Internal n8n service (Docker network)
    # - 'https://n8n.${DOMAIN}'  # External n8n URL (production)

# MCP Calculator Server
mcpServers:
  calculator:
    type: streamable-http
    url: http://mcp-calculator:3012/mcp
    title: Rechner
    description: Mathematische Berechnungen und Rechenoperationen
    iconPath: /images/mcp-calculator-icon.svg
    initTimeout: 120000
    chatMenu: false
    startup: true
    serverInstructions: true

  image-gen:
    type: streamable-http
    url: http://mcp-image-gen:3013/mcp
    title: Bildgenerierung
    description: Bilder aus Textbeschreibungen erstellen
    iconPath: /images/mcp-image-gen-icon.svg
    initTimeout: 180000
    timeout: 150000  # 150 seconds for tool calls (image generation can take up to 120s)
    chatMenu: true
    startup: true
    serverInstructions: true

  openstreetmap:
    type: streamable-http
    url: http://mcp-openstreetmap:3004/mcp
    title: OpenStreetMap
    description: Geografische Suche, Routenplanung und Standortinformationen
    iconPath: /images/mcp-openstreetmap-icon.svg
    initTimeout: 120000
    chatMenu: false
    startup: true
    serverInstructions: true

  weather:
    type: streamable-http
    url: http://mcp-weather:3005/mcp
    title: Wetter
    description: Aktuelle Wetterdaten, Luftqualität und Zeitzoneninformationen
    iconPath: /images/mcp-weather-icon.svg
    initTimeout: 120000
    chatMenu: true
    startup: true
    serverInstructions: true

  docs:
    type: streamable-http
    url: http://mcp-docs:6280/mcp
    title: Dokumentation durchsuchen
    description: Öffentlich verfügbare Dokumentation durchsuchen
    iconPath: /images/mcp-docs-icon.svg
    initTimeout: 120000
    chatMenu: true
    startup: true
    serverInstructions: true

  playwright:
    type: streamable-http
    url: http://mcp-playwright:3006/mcp
    title: Browser Automatisierung
    description: Webseiten durchsuchen, interagieren und automatisieren
    iconPath: /images/mcp-playwright-icon.svg
    initTimeout: 120000
    timeout: 150000
    chatMenu: false
    startup: false # Currently experimental, maybe not needed in favor of default websearch feature
    serverInstructions: true

  db-timetable:
    type: streamable-http
    url: http://mcp-db-timetable:3007/mcp
    title: DB Fahrplan
    description: Aktuelle Fahrpläne, Stationssuche und Zugverbindungen der Deutschen Bahn
    iconPath: /images/mcp-db-timetable-icon.svg
    initTimeout: 120000
    chatMenu: true
    startup: true
    serverInstructions: true

  stackoverflow:
    type: streamable-http
    url: http://mcp-stackoverflow:3008/mcp
    title: Stack Overflow
    description: Programmierlösungen und Fehlerbehebung finden
    iconPath: /images/mcp-stackoverflow-icon.svg
    initTimeout: 120000
    chatMenu: false
    startup: true
    serverInstructions: true

  npm-search:
    type: streamable-http
    url: http://mcp-npm-search:3009/mcp
    title: npm Suche
    description: npm-Pakete durchsuchen und finden
    iconPath: /images/mcp-npm-search-icon.svg
    initTimeout: 120000
    chatMenu: false
    startup: true
    serverInstructions: true

  chefkoch:
    type: streamable-http
    url: http://mcp-chefkoch:3014/mcp
    title: Rezepte
    description: Rezepte von Chefkoch.de suchen und anzeigen
    iconPath: /images/mcp-chefkoch-icon.svg
    initTimeout: 120000
    chatMenu: false
    startup: true
    serverInstructions: true

  ytptube:
    type: streamable-http
    url: http://mcp-ytptube:3010/mcp
    title: Video Transkripte
    description: Transkripte von Videos anfordern und Status prüfen.
    iconPath: /images/mcp-ytptube-icon.svg
    initTimeout: 120000
    timeout: 300000   # 5 min — downloads + STT
    chatMenu: false
    startup: false # Currently experimental
    serverInstructions: true

  youtube-transcript:
    type: streamable-http
    url: http://mcp-youtube-transcript:3011/mcp
    title: YouTube Transkript
    description: Transkripte von YouTube-Videos abrufen
    iconPath: /images/mcp-youtube-transcript-icon.svg
    initTimeout: 120000
    chatMenu: false
    startup: false # Currently experimental, mybe not needed in favor of ytptube
    serverInstructions: true

  github:
    type: streamable-http
    url: https://api.githubcopilot.com/mcp/
    title: GitHub
    description: Repositories, Issues, Pull Requests und Code-Analyse
    iconPath: /images/mcp-github-icon.svg
    headers:
      Authorization: "Bearer ${MCP_GITHUB_PAT}"
      X-MCP-Readonly: "true"
    initTimeout: 120000
    chatMenu: true
    startup: true
    serverInstructions: true

  mapbox:
    type: streamable-http
    url: https://mcp.mapbox.com/mcp
    title: Mapbox
    description: Geografische Suche, Routenplanung, Geocoding und Kartenvisualisierung
    iconPath: /images/mcp-mapbox-icon.svg
    headers:
      Authorization: "Bearer ${MCP_MAPBOX_ACCESS_TOKEN}"
    initTimeout: 120000
    timeout: 150000
    chatMenu: true
    startup: true
    serverInstructions: true

  # Web scraping (scrape, search, map, crawl, extract). Not shown in chat dropdown; available for agents.
  # firecrawl:
  #   type: streamable-http
  #   url: http://mcp-firecrawl:3003/mcp
  #   title: Web Scraping
  #   description: Webseiten scrapen, durchsuchen und analysieren
  #   iconPath: /images/mcp-firecrawl-icon.svg
  #   initTimeout: 120000
  #   chatMenu: false
  #   startup: false
  #   serverInstructions: true

# n8n MCP Server Integration (Future - disabled)
# To integrate n8n workflows with LibreChat Agents via MCP:
# 1. Create an HTTP-based MCP server bridge that translates MCP calls to n8n webhook requests
# 2. Configure the MCP server below as a streamable-http type
# 3. Add n8n domain to mcpSettings.allowedDomains (see above)
# Example configuration (uncomment when ready):
#   n8n:
#     type: streamable-http
#     url: http://n8n:5678/mcp  # Internal Docker network URL
#     # Or use external URL: https://n8n.${DOMAIN}/mcp
#     headers:
#       X-User-ID: "{{LIBRECHAT_USER_ID}}"
#       Authorization: "Bearer ${N8N_API_KEY}"
#     timeout: 30000
#     serverInstructions: true
#     chatMenu: true  # Show in chat dropdown (or false for agents only)

# OCR Configuration (Optical Character Recognition)
# Requires: LIBRECHAT_OCR_API_KEY environment variable set with your Mistral API key
ocr:
  mistralModel: "mistral-ocr-latest"
  apiKey: "${OCR_API_KEY}"
  baseURL: "${OCR_BASEURL}"
  strategy: "mistral_ocr"

# File Configuration - Define supported MIME types for OCR and other file operations
fileConfig:
  ocr:
    # Mistral OCR supports: PNG, JPEG/JPG, AVIF (images) + PDF, DOCX, PPTX (documents)
    # Using image/.* to allow all image formats - unsupported ones will fall back to text parsing
    supportedMimeTypes:
      - "image/.*"
      - "application/pdf"
      - "application/vnd.openxmlformats-officedocument.wordprocessingml.document"  # DOCX
      - "application/vnd.openxmlformats-officedocument.presentationml.presentation"  # PPTX
      - "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"  # XLSX
  text:
    # All text/* MIME types (covers: plain, javascript, python, html, markdown, css, csv, etc.)
    # Plus specific application/* MIME types for text-based formats
    supportedMimeTypes:
      - "text/.*"
      - "application/json"
      - "application/yaml"
      - "application/csv"
      - "application/typescript"
      - "application/sql"
      - "application/xml"
      - "application/x-sh"
      - "application/vnd.coffeescript"
  stt:
    # Audio formats supported by Azure OpenAI STT and other STT providers
    # Formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, webm, aac, opus, wma
    supportedMimeTypes:
      - "^audio/(mp3|mpeg|mpeg3|mp4|m4a|mpga|wav|wave|x-wav|ogg|vorbis|webm|flac|x-flac|aac|wma|opus)$"

# File Strategy (Granular) - disabled (default: "local" for all file types)
# Allows different storage strategies for different file types
# If not specified, all file types default to "local"
# Priority: specific type > fileStrategies.default > fileStrategy > "local"
# Available strategies: "local", "s3", "firebase", "azure_blob"
fileStrategies:
  default: "local"
  avatar: "local"
  image: "local"
  document: "local"
